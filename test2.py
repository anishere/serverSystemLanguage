import cv2
import pytesseract
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import openai
import torch
import concurrent.futures
from segment_anything import SamPredictor, sam_model_registry

# üîπ C·∫•u h√¨nh OpenAI API (>1.0)
client = openai.OpenAI(api_key="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")

# üîπ C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n Tesseract
pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

# üîπ Load m√¥ h√¨nh SAM ƒë·ªÉ t√°ch n·ªÅn vƒÉn b·∫£n
sam_checkpoint = "sam_vit_h_4b8939.pth"
sam = sam_model_registry["vit_h"](checkpoint=sam_checkpoint)
predictor = SamPredictor(sam)
sam.to("cuda" if torch.cuda.is_available() else "cpu")

# üîπ Ti·ªÅn x·ª≠ l√Ω ·∫£nh ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c OCR
def preprocess_image(image_path):
    image = cv2.imread(image_path)
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    processed = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                      cv2.THRESH_BINARY, 31, 10)
    kernel = np.ones((1,1), np.uint8)
    processed = cv2.morphologyEx(processed, cv2.MORPH_CLOSE, kernel)
    return processed

# üîπ T√°ch ch·ªØ b·∫±ng m√¥ h√¨nh SAM
def segment_text(image_path):
    image = cv2.imread(image_path)
    predictor.set_image(image)
    masks, _, _ = predictor.predict()
    return masks

# üîπ Nh·∫≠n di·ªán vƒÉn b·∫£n & v·ªã tr√≠ bounding box t·ª´ ·∫£nh
def extract_text_with_boxes(image_path):
    image = preprocess_image(image_path)
    data = pytesseract.image_to_data(image, lang="vie+eng", output_type=pytesseract.Output.DICT)

    text_boxes = []
    for i in range(len(data["text"])):
        if int(data["conf"][i]) > 60:  # Ch·ªâ l·∫•y vƒÉn b·∫£n c√≥ ƒë·ªô tin c·∫≠y cao
            x, y, w, h = data["left"][i], data["top"][i], data["width"][i], data["height"][i]
            text_boxes.append({"text": data["text"][i], "bbox": (x, y, x + w, y + h)})

    return text_boxes

# üîπ D·ªãch vƒÉn b·∫£n b·∫±ng OpenAI GPT-4o Mini (Ch·∫°y ƒëa lu·ªìng)
def translate_text(text, target_language="Vietnamese"):
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": (
                    "You are a helpful assistant that translates text. "
                    "Always return only the translated text without any additional explanations."
                )
            },
            {
                "role": "user",
                "content": f"Translate this text from Auto to {target_language}: {text}"
            }
        ],
        max_tokens=100
    )
    return response.choices[0].message.content.strip()

# üîπ D·ªãch to√†n b·ªô vƒÉn b·∫£n b·∫±ng ƒêa Lu·ªìng (TƒÉng s·ªë lu·ªìng)
def translate_text_bulk(text_list, target_language="Vietnamese"):
    translated_texts = []
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:  # TƒÉng s·ªë lu·ªìng ƒë·ªÉ d·ªãch nhanh h∆°n
        future_to_text = {executor.submit(translate_text, text, target_language): text for text in text_list}
        for future in concurrent.futures.as_completed(future_to_text):
            try:
                translated_texts.append(future.result())
            except Exception as e:
                translated_texts.append("")  # N·∫øu l·ªói, tr·∫£ v·ªÅ chu·ªói r·ªóng

    return translated_texts

# üîπ M·ªü r·ªông k√≠ch th∆∞·ªõc ·∫£nh n·∫øu vƒÉn b·∫£n d·ªãch d√†i h∆°n nhi·ªÅu
def expand_image_if_needed(img, text_boxes, translated_texts, font_path="arial.ttf"):
    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    font = ImageFont.truetype(font_path, 30)  # Gi·ªØ nguy√™n k√≠ch th∆∞·ªõc ch·ªØ

    max_width = img_pil.width
    expand_needed = False

    for box, translated_text in zip(text_boxes, translated_texts):
        x_min, y_min, x_max, y_max = box["bbox"]
        text_width, _ = draw.textbbox((0, 0), translated_text, font=font)[2:4]

        if text_width > (x_max - x_min):
            expand_needed = True
            max_width = max(max_width, x_min + text_width + 10)

    if expand_needed:
        new_img = Image.new("RGB", (max_width, img_pil.height), "white")
        new_img.paste(img_pil, (0, 0))
        img = cv2.cvtColor(np.array(new_img), cv2.COLOR_RGB2BGR)
        return img
    return img

# üîπ ƒê√® vƒÉn b·∫£n d·ªãch l√™n ·∫£nh g·ªëc
# üîπ ƒê√® vƒÉn b·∫£n d·ªãch l√™n ·∫£nh g·ªëc
# üîπ T√≠nh to√°n font size t·ª± ƒë·ªông ƒë·ªÉ tr√°nh ch·ªØ ƒë√® l√™n nhau
def get_optimal_fontsize(text, bbox_width, bbox_height, font_path="arial.ttf"):
    """ ƒêi·ªÅu ch·ªânh k√≠ch th∆∞·ªõc font ƒë·ªÉ v·ª´a v·ªõi bounding box """
    font_size = bbox_height  # B·∫Øt ƒë·∫ßu b·∫±ng k√≠ch th∆∞·ªõc bbox
    font = ImageFont.truetype(font_path, font_size)

    while font.getbbox(text)[2] > bbox_width and font_size > 10:
        font_size -= 1
        font = ImageFont.truetype(font_path, font_size)

    return font


# üîπ ƒê√® vƒÉn b·∫£n d·ªãch l√™n ·∫£nh g·ªëc
def overlay_translated_text(image_path, output_path, target_language="Vietnamese"):
    text_boxes = extract_text_with_boxes(image_path)
    
    # L·∫•y to√†n b·ªô vƒÉn b·∫£n c·∫ßn d·ªãch
    original_texts = [box["text"] for box in text_boxes]
    
    # S·ª≠ d·ª•ng ƒëa lu·ªìng ƒë·ªÉ d·ªãch nhanh h∆°n
    translated_texts = translate_text_bulk(original_texts, target_language)
    
    # ƒê·ªçc ·∫£nh b·∫±ng OpenCV v√† chuy·ªÉn sang Pillow ƒë·ªÉ x·ª≠ l√Ω text overlay
    img = cv2.imread(image_path)

    # Ki·ªÉm tra xem c√≥ c·∫ßn m·ªü r·ªông ·∫£nh kh√¥ng
    img = expand_image_if_needed(img, text_boxes, translated_texts)

    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    font_path = "arial.ttf"

    # S·ª≠ d·ª•ng m√¥ h√¨nh SAM ƒë·ªÉ t√°ch n·ªÅn ch·ªØ c≈©
    masks = segment_text(image_path)
    for mask in masks:
        resized_mask = cv2.resize(mask.astype(np.uint8) * 255, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)
        img[resized_mask > 0] = 255  # L√†m tr·∫Øng n·ªÅn t·∫°i v·ªã tr√≠ c√≥ ch·ªØ

    # ƒê√® vƒÉn b·∫£n d·ªãch l√™n ·∫£nh v·ªõi font size t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh
    for box, translated_text in zip(text_boxes, translated_texts):
        x_min, y_min, x_max, y_max = box["bbox"]
        bbox_width = x_max - x_min
        bbox_height = y_max - y_min

        print(f"Original: {box['text']} -> Translated: {translated_text}")

        # L·∫•y font size ph√π h·ª£p
        font = get_optimal_fontsize(translated_text, bbox_width, bbox_height, font_path)

        # V·∫Ω n·ªÅn tr·∫Øng tr∆∞·ªõc khi v·∫Ω ch·ªØ m·ªõi
        draw.rectangle([x_min, y_min, x_max, y_max], fill="white")

        # V·∫Ω vƒÉn b·∫£n d·ªãch
        draw.text((x_min, y_min), translated_text, font=font, fill="black")

    # L∆∞u ·∫£nh cu·ªëi c√πng
    final_img = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
    cv2.imwrite(output_path, final_img)
    print(f"‚úÖ ·∫¢nh ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {output_path}")

# üîπ Ch·∫°y th·ª≠ nghi·ªám
if __name__ == "__main__":
    input_image = "C:/Users/acer/Pictures/Screenshots/Screenshot 2025-01-26 105248.png"
    output_image = "translated_overlay_sam.png"

    overlay_translated_text(input_image, output_image, "Vietnamese")
